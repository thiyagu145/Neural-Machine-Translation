{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NMT-1.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"lEyw55Bh-a0l","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":136},"outputId":"1bf83cf8-71cb-408d-fe5f-5e3854e2e67a","executionInfo":{"status":"ok","timestamp":1527848559535,"user_tz":-330,"elapsed":25220,"user":{"displayName":"Thiyagarajan Ramanathan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"111432628313695784552"}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["gpg: keybox '/tmp/tmpob8ugcsj/pubring.gpg' created\n","gpg: /tmp/tmpob8ugcsj/trustdb.gpg: trustdb created\n","gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","··········\n"],"name":"stdout"}]},{"metadata":{"id":"fEmMtiMm_Qk0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":51},"outputId":"3bb8641f-5195-4606-c020-7f65256c468a","executionInfo":{"status":"ok","timestamp":1527848567121,"user_tz":-330,"elapsed":5033,"user":{"displayName":"Thiyagarajan Ramanathan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"111432628313695784552"}}},"cell_type":"code","source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":2,"outputs":[{"output_type":"stream","text":["fuse: mountpoint is not empty\r\n","fuse: if you are sure this is safe, use the 'nonempty' mount option\r\n"],"name":"stdout"}]},{"metadata":{"id":"6YohpGgO_VGe","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# split a loaded document into sentences\n","def to_pairs(doc):\n","\tlines = doc.strip().split('\\n')\n","\tpairs = [line.split('\\t') for line in  lines]\n","\treturn pairs\n","\n","# clean a list of lines\n","def clean_pairs(lines):\n","\tcleaned = list()\n","\t# prepare regex for char filtering\n","\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor pair in lines:\n","\t\tclean_pair = list()\n","\t\tfor line in pair:\n","\t\t\t# normalize unicode characters\n","\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n","\t\t\tline = line.decode('UTF-8')\n","\t\t\t# tokenize on white space\n","\t\t\tline = line.split()\n","\t\t\t# convert to lowercase\n","\t\t\tline = [word.lower() for word in line]\n","\t\t\t# remove punctuation from each token\n","\t\t\tline = [word.translate(table) for word in line]\n","\t\t\t# remove non-printable chars form each token\n","\t\t\tline = [re_print.sub('', w) for w in line]\n","\t\t\t# remove tokens with numbers in them\n","\t\t\tline = [word for word in line if word.isalpha()]\n","\t\t\t# store as string\n","\t\t\tclean_pair.append(' '.join(line))\n","\t\tcleaned.append(clean_pair)\n","\treturn array(cleaned)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gJqV03LhAQlo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"954911ea-8370-4c10-d2b8-535ba49ad3f9","executionInfo":{"status":"ok","timestamp":1527508058108,"user_tz":-330,"elapsed":389860,"user":{"displayName":"Thiyagarajan Ramanathan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"111432628313695784552"}}},"cell_type":"code","source":["import string\n","import re\n","from pickle import dump\n","from unicodedata import normalize\n","from numpy import array\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, mode='rt', encoding='utf-8')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# split a loaded document into sentences\n","def to_pairs(doc):\n","\tlines = doc.strip().split('\\n')\n","\tpairs = [line.split('\\t') for line in  lines]\n","\treturn pairs\n","\n","# clean a list of lines\n","def clean_pairs(lines):\n","\tcleaned = list()\n","\t# prepare regex for char filtering\n","\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor pair in lines:\n","\t\tclean_pair = list()\n","\t\tfor line in pair:\n","\t\t\t# normalize unicode characters\n","\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n","\t\t\tline = line.decode('UTF-8')\n","\t\t\t# tokenize on white space\n","\t\t\tline = line.split()\n","\t\t\t# convert to lowercase\n","\t\t\tline = [word.lower() for word in line]\n","\t\t\t# remove punctuation from each token\n","\t\t\tline = [word.translate(table) for word in line]\n","\t\t\t# remove non-printable chars form each token\n","\t\t\tline = [re_print.sub('', w) for w in line]\n","\t\t\t# remove tokens with numbers in them\n","\t\t\tline = [word for word in line if word.isalpha()]\n","\t\t\t# store as string\n","\t\t\tclean_pair.append(' '.join(line))\n","\t\tcleaned.append(clean_pair)\n","\treturn array(cleaned)\n","\n","# save a list of clean sentences to file\n","def save_clean_data(sentences, filename):\n","\tdump(sentences, open(filename, 'wb'))\n","\tprint('Saved: %s' % filename)\n","\n","# load dataset\n","filename = 'drive/Colab Notebooks/vietnamese.txt'\n","doc = load_doc(filename)\n","# split into english-german pairs\n","pairs = to_pairs(doc)\n","# clean sentences\n","clean_pairs = clean_pairs(pairs)\n","# save clean pairs to file\n","save_clean_data(clean_pairs, 'drive/Colab Notebooks/english-vietnamese.pkl')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Saved: drive/Colab Notebooks/english-vietnamese.pkl\n"],"name":"stdout"}]},{"metadata":{"id":"AEoCQsQn71M-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":68},"outputId":"2a84a98a-2752-4ae2-f3b0-67fc14c4ac13","executionInfo":{"status":"ok","timestamp":1527848637545,"user_tz":-330,"elapsed":10829,"user":{"displayName":"Thiyagarajan Ramanathan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"111432628313695784552"}}},"cell_type":"code","source":["from pickle import load\n","from pickle import dump\n","from numpy.random import rand\n","from numpy.random import shuffle\n","\n","# load a clean dataset\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n","\n","# save a list of clean sentences to file\n","def save_clean_data(sentences, filename):\n","\tdump(sentences, open(filename, 'wb'))\n","\tprint('Saved: %s' % filename)\n","\n","# load dataset\n","raw_dataset = load_clean_sentences('drive/Colab Notebooks/english-vietnamese.pkl')\n","\n","# reduce dataset size\n","n_sentences = 15000\n","dataset = raw_dataset[:n_sentences, :]\n","\n","# random shuffle\n","shuffle(dataset)\n","# split into train/test\n","train, test = dataset[:13000], dataset[13000:]\n","# save\n","save_clean_data(dataset, 'drive/Colab Notebooks/english-vietnamese-both.pkl')\n","save_clean_data(train, 'drive/Colab Notebooks/english-vietnamese-train.pkl')\n","save_clean_data(test, 'drive/Colab Notebooks/english-vietnamese-test.pkl')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Saved: drive/Colab Notebooks/english-vietnamese-both.pkl\n","Saved: drive/Colab Notebooks/english-vietnamese-train.pkl\n","Saved: drive/Colab Notebooks/english-vietnamese-test.pkl\n"],"name":"stdout"}]},{"metadata":{"id":"uyTmqGoYXSYN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# load a clean dataset\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n","\n","# load datasets\n","dataset = load_clean_sentences('drive/Colab Notebooks/english-chinese-both.pkl')\n","train = load_clean_sentences('drive/Colab Notebooks/english-chinese-train.pkl')\n","test = load_clean_sentences('drive/Colab Notebooks/english-chinese-test.pkl')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m2FI3DfdXrqh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# fit a tokenizer\n","def create_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"87Akcv1kXt4i","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# max sentence length\n","def max_length(lines):\n","\treturn max(len(line.split()) for line in lines)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CIp5ChbWXwQL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":85},"outputId":"334a6b2c-89e5-46ba-bb30-1f2ff411cb77","executionInfo":{"status":"ok","timestamp":1527848641352,"user_tz":-330,"elapsed":1443,"user":{"displayName":"Thiyagarajan Ramanathan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"111432628313695784552"}}},"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","# prepare english tokenizer\n","eng_tokenizer = create_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:, 0])\n","print('English Vocabulary Size: %d' % eng_vocab_size)\n","print('English Max Length: %d' % (eng_length))\n","# prepare german tokenizer\n","ger_tokenizer = create_tokenizer(dataset[:, 1])\n","ger_vocab_size = len(ger_tokenizer.word_index) + 1\n","ger_length = max_length(dataset[:, 1])\n","print('chinese Vocabulary Size: %d' % ger_vocab_size)\n","print('chinese Max Length: %d' % (ger_length))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["English Vocabulary Size: 7497\n","English Max Length: 9\n","chinese Vocabulary Size: 4233\n","chinese Max Length: 18\n"],"name":"stdout"}]},{"metadata":{"id":"slJRxVwQXy_Z","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","# encode and pad sequences\n","from keras.preprocessing.sequence import pad_sequences\n","def encode_sequences(tokenizer, length, lines):\n","\t# integer encode sequences\n","\tX = tokenizer.texts_to_sequences(lines)\n","\t# pad sequences with 0 values\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_IrkTNSBX3up","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from array import array\n","import numpy as np\n","# one hot encode target sequence\n","from keras.utils import to_categorical\n","def encode_output(sequences, vocab_size):\n","\tylist = list()\n","\tfor sequence in sequences:\n","\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n","\t\tylist.append(encoded)\n","\ty = np.array(ylist)\n","\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n","\treturn y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Qyy11JOCX5wC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# prepare training data\n","trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n","trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n","trainY = encode_output(trainY, eng_vocab_size)\n","# prepare validation data\n","testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n","testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n","testY = encode_output(testY, eng_vocab_size)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wWTrXsSUYQhl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":54},"outputId":"9ff8d936-ea28-4a7f-a631-c3babb39abcc","executionInfo":{"status":"ok","timestamp":1527854718312,"user_tz":-330,"elapsed":2021,"user":{"displayName":"Thiyagarajan Ramanathan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"111432628313695784552"}}},"cell_type":"code","source":["import tensorflow as tf\n","from keras import backend as K\n","from keras import regularizers, constraints, initializers, activations\n","from keras.layers.recurrent import Recurrent\n","from keras.engine import InputSpec\n","\n","tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n","\n","def _time_distributed_dense(x, w, b=None, dropout=None,\n","                            input_dim=None, output_dim=None,\n","                            timesteps=None, training=None):\n","    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n","    # Arguments\n","        x: input tensor.\n","        w: weight matrix.\n","        b: optional bias vector.\n","        dropout: wether to apply dropout (same dropout mask\n","            for every temporal slice of the input).\n","        input_dim: integer; optional dimensionality of the input.\n","        output_dim: integer; optional dimensionality of the output.\n","        timesteps: integer; optional number of timesteps.\n","        training: training phase tensor or boolean.\n","    # Returns\n","        Output tensor.\n","    \"\"\"\n","    if not input_dim:\n","        input_dim = K.shape(x)[2]\n","    if not timesteps:\n","        timesteps = K.shape(x)[1]\n","    if not output_dim:\n","        output_dim = K.shape(w)[1]\n","\n","    if dropout is not None and 0. < dropout < 1.:\n","        # apply the same dropout pattern at every timestep\n","        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n","        dropout_matrix = K.dropout(ones, dropout)\n","        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n","        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n","\n","    # collapse time dimension and batch dimension together\n","    x = K.reshape(x, (-1, input_dim))\n","    x = K.dot(x, w)\n","    if b is not None:\n","        x = K.bias_add(x, b)\n","    # reshape to 3D tensor\n","    if K.backend() == 'tensorflow':\n","        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n","        x.set_shape([None, None, output_dim])\n","    else:\n","        x = K.reshape(x, (-1, timesteps, output_dim))\n","    return x\n","\n","class AttentionDecoder(Recurrent):\n","\n","    def __init__(self, units, output_dim,\n","                 activation='tanh',\n","                 return_probabilities=False,\n","                 name='AttentionDecoder',\n","                 kernel_initializer='glorot_uniform',\n","                 recurrent_initializer='orthogonal',\n","                 bias_initializer='zeros',\n","                 kernel_regularizer=None,\n","                 bias_regularizer=None,\n","                 activity_regularizer=None,\n","                 kernel_constraint=None,\n","                 bias_constraint=None,\n","                 **kwargs):\n","        \"\"\"\n","        Implements an AttentionDecoder that takes in a sequence encoded by an\n","        encoder and outputs the decoded states \n","        :param units: dimension of the hidden state and the attention matrices\n","        :param output_dim: the number of labels in the output space\n","        references:\n","            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \n","            \"Neural machine translation by jointly learning to align and translate.\" \n","            arXiv preprint arXiv:1409.0473 (2014).\n","        \"\"\"\n","        self.units = units\n","        self.output_dim = output_dim\n","        self.return_probabilities = return_probabilities\n","        self.activation = activations.get(activation)\n","        self.kernel_initializer = initializers.get(kernel_initializer)\n","        self.recurrent_initializer = initializers.get(recurrent_initializer)\n","        self.bias_initializer = initializers.get(bias_initializer)\n","\n","        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n","        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n","        self.bias_regularizer = regularizers.get(bias_regularizer)\n","        self.activity_regularizer = regularizers.get(activity_regularizer)\n","\n","        self.kernel_constraint = constraints.get(kernel_constraint)\n","        self.recurrent_constraint = constraints.get(kernel_constraint)\n","        self.bias_constraint = constraints.get(bias_constraint)\n","\n","        super(AttentionDecoder, self).__init__(**kwargs)\n","        self.name = name\n","        self.return_sequences = True  # must return sequences\n","\n","    def build(self, input_shape):\n","        \"\"\"\n","          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n","          for model details that correspond to the matrices here.\n","        \"\"\"\n","\n","        self.batch_size, self.timesteps, self.input_dim = input_shape\n","\n","        if self.stateful:\n","            super(AttentionDecoder, self).reset_states()\n","\n","        self.states = [None, None]  # y, s\n","\n","        \"\"\"\n","            Matrices for creating the context vector\n","        \"\"\"\n","\n","        self.V_a = self.add_weight(shape=(self.units,),\n","                                   name='V_a',\n","                                   initializer=self.kernel_initializer,\n","                                   regularizer=self.kernel_regularizer,\n","                                   constraint=self.kernel_constraint)\n","        self.W_a = self.add_weight(shape=(self.units, self.units),\n","                                   name='W_a',\n","                                   initializer=self.kernel_initializer,\n","                                   regularizer=self.kernel_regularizer,\n","                                   constraint=self.kernel_constraint)\n","        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n","                                   name='U_a',\n","                                   initializer=self.kernel_initializer,\n","                                   regularizer=self.kernel_regularizer,\n","                                   constraint=self.kernel_constraint)\n","        self.b_a = self.add_weight(shape=(self.units,),\n","                                   name='b_a',\n","                                   initializer=self.bias_initializer,\n","                                   regularizer=self.bias_regularizer,\n","                                   constraint=self.bias_constraint)\n","        \"\"\"\n","            Matrices for the r (reset) gate\n","        \"\"\"\n","        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n","                                   name='C_r',\n","                                   initializer=self.recurrent_initializer,\n","                                   regularizer=self.recurrent_regularizer,\n","                                   constraint=self.recurrent_constraint)\n","        self.U_r = self.add_weight(shape=(self.units, self.units),\n","                                   name='U_r',\n","                                   initializer=self.recurrent_initializer,\n","                                   regularizer=self.recurrent_regularizer,\n","                                   constraint=self.recurrent_constraint)\n","        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n","                                   name='W_r',\n","                                   initializer=self.recurrent_initializer,\n","                                   regularizer=self.recurrent_regularizer,\n","                                   constraint=self.recurrent_constraint)\n","        self.b_r = self.add_weight(shape=(self.units, ),\n","                                   name='b_r',\n","                                   initializer=self.bias_initializer,\n","                                   regularizer=self.bias_regularizer,\n","                                   constraint=self.bias_constraint)\n","\n","        \"\"\"\n","            Matrices for the z (update) gate\n","        \"\"\"\n","        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n","                                   name='C_z',\n","                                   initializer=self.recurrent_initializer,\n","                                   regularizer=self.recurrent_regularizer,\n","                                   constraint=self.recurrent_constraint)\n","        self.U_z = self.add_weight(shape=(self.units, self.units),\n","                                   name='U_z',\n","                                   initializer=self.recurrent_initializer,\n","                                   regularizer=self.recurrent_regularizer,\n","                                   constraint=self.recurrent_constraint)\n","        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n","                                   name='W_z',\n","                                   initializer=self.recurrent_initializer,\n","                                   regularizer=self.recurrent_regularizer,\n","                                   constraint=self.recurrent_constraint)\n","        self.b_z = self.add_weight(shape=(self.units, ),\n","                                   name='b_z',\n","                                   initializer=self.bias_initializer,\n","                                   regularizer=self.bias_regularizer,\n","                                   constraint=self.bias_constraint)\n","        \"\"\"\n","            Matrices for the proposal\n","        \"\"\"\n","        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n","                                   name='C_p',\n","                                   initializer=self.recurrent_initializer,\n","                                   regularizer=self.recurrent_regularizer,\n","                                   constraint=self.recurrent_constraint)\n","        self.U_p = self.add_weight(shape=(self.units, self.units),\n","                                   name='U_p',\n","                                   initializer=self.recurrent_initializer,\n","                                   regularizer=self.recurrent_regularizer,\n","                                   constraint=self.recurrent_constraint)\n","        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n","                                   name='W_p',\n","                                   initializer=self.recurrent_initializer,\n","                                   regularizer=self.recurrent_regularizer,\n","                                   constraint=self.recurrent_constraint)\n","        self.b_p = self.add_weight(shape=(self.units, ),\n","                                   name='b_p',\n","                                   initializer=self.bias_initializer,\n","                                   regularizer=self.bias_regularizer,\n","                                   constraint=self.bias_constraint)\n","        \"\"\"\n","            Matrices for making the final prediction vector\n","        \"\"\"\n","        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n","                                   name='C_o',\n","                                   initializer=self.recurrent_initializer,\n","                                   regularizer=self.recurrent_regularizer,\n","                                   constraint=self.recurrent_constraint)\n","        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n","                                   name='U_o',\n","                                   initializer=self.recurrent_initializer,\n","                                   regularizer=self.recurrent_regularizer,\n","                                   constraint=self.recurrent_constraint)\n","        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n","                                   name='W_o',\n","                                   initializer=self.recurrent_initializer,\n","                                   regularizer=self.recurrent_regularizer,\n","                                   constraint=self.recurrent_constraint)\n","        self.b_o = self.add_weight(shape=(self.output_dim, ),\n","                                   name='b_o',\n","                                   initializer=self.bias_initializer,\n","                                   regularizer=self.bias_regularizer,\n","                                   constraint=self.bias_constraint)\n","\n","        # For creating the initial state:\n","        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n","                                   name='W_s',\n","                                   initializer=self.recurrent_initializer,\n","                                   regularizer=self.recurrent_regularizer,\n","                                   constraint=self.recurrent_constraint)\n","\n","        self.input_spec = [\n","            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n","        self.built = True\n","\n","    def call(self, x):\n","        # store the whole sequence so we can \"attend\" to it at each timestep\n","        self.x_seq = x\n","\n","        # apply the a dense layer over the time dimension of the sequence\n","        # do it here because it doesn't depend on any previous steps\n","        # thefore we can save computation time:\n","        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n","                                             input_dim=self.input_dim,\n","                                             timesteps=self.timesteps,\n","                                             output_dim=self.units)\n","\n","        return super(AttentionDecoder, self).call(x)\n","\n","    def get_initial_state(self, inputs):\n","        print('inputs shape:', inputs.get_shape())\n","\n","        # apply the matrix on the first time step to get the initial s0.\n","        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n","\n","        # from keras.layers.recurrent to initialize a vector of (batchsize,\n","        # output_dim)\n","        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n","        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n","        y0 = K.expand_dims(y0)  # (samples, 1)\n","        y0 = K.tile(y0, [1, self.output_dim])\n","\n","        return [y0, s0]\n","\n","    def step(self, x, states):\n","\n","        ytm, stm = states\n","\n","        # repeat the hidden state to the length of the sequence\n","        _stm = K.repeat(stm, self.timesteps)\n","\n","        # now multiplty the weight matrix with the repeated hidden state\n","        _Wxstm = K.dot(_stm, self.W_a)\n","\n","        # calculate the attention probabilities\n","        # this relates how much other timesteps contributed to this one.\n","        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n","                   K.expand_dims(self.V_a))\n","        at = K.exp(et)\n","        at_sum = K.sum(at, axis=1)\n","        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n","        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n","\n","        # calculate the context vector\n","        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n","        # ~~~> calculate new hidden state\n","        # first calculate the \"r\" gate:\n","\n","        rt = activations.sigmoid(\n","            K.dot(ytm, self.W_r)\n","            + K.dot(stm, self.U_r)\n","            + K.dot(context, self.C_r)\n","            + self.b_r)\n","\n","        # now calculate the \"z\" gate\n","        zt = activations.sigmoid(\n","            K.dot(ytm, self.W_z)\n","            + K.dot(stm, self.U_z)\n","            + K.dot(context, self.C_z)\n","            + self.b_z)\n","\n","        # calculate the proposal hidden state:\n","        s_tp = activations.tanh(\n","            K.dot(ytm, self.W_p)\n","            + K.dot((rt * stm), self.U_p)\n","            + K.dot(context, self.C_p)\n","            + self.b_p)\n","\n","        # new hidden state:\n","        st = (1-zt)*stm + zt * s_tp\n","\n","        yt = activations.softmax(\n","            K.dot(ytm, self.W_o)\n","            + K.dot(stm, self.U_o)\n","            + K.dot(context, self.C_o)\n","            + self.b_o)\n","\n","        if self.return_probabilities:\n","            return at, [yt, st]\n","        else:\n","            return yt, [yt, st]\n","\n","    def compute_output_shape(self, input_shape):\n","        \"\"\"\n","            For Keras internal compatability checking\n","        \"\"\"\n","        if self.return_probabilities:\n","            return (None, self.timesteps, self.timesteps)\n","        else:\n","            return (None, self.timesteps, self.output_dim)\n","\n","    def get_config(self):\n","        \"\"\"\n","            For rebuilding models on load time.\n","        \"\"\"\n","        config = {\n","            'output_dim': self.output_dim,\n","            'units': self.units,\n","            'return_probabilities': self.return_probabilities\n","        }\n","        base_config = super(AttentionDecoder, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\"\"\"\n","# check to see if it compiles\n","if __name__ == '__main__':\n","    from keras.layers import Input, LSTM\n","    from keras.models import Model\n","    from keras.layers.wrappers import Bidirectional\n","    i = Input(shape=(100,104), dtype='float32')\n","    enc = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(i)\n","    dec = AttentionDecoder(32, 4)(enc)\n","    model = Model(inputs=i, outputs=dec)\n","    model.summary()\n","\"\"\"\n"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# check to see if it compiles\\nif __name__ == '__main__':\\n    from keras.layers import Input, LSTM\\n    from keras.models import Model\\n    from keras.layers.wrappers import Bidirectional\\n    i = Input(shape=(100,104), dtype='float32')\\n    enc = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(i)\\n    dec = AttentionDecoder(32, 4)(enc)\\n    model = Model(inputs=i, outputs=dec)\\n    model.summary()\\n\""]},"metadata":{"tags":[]},"execution_count":38}]},{"metadata":{"id":"8WqPPWoFX8eY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from keras import backend as K\n","import os\n","import importlib\n","\n","def set_keras_backend(backend):\n","\n","    if K.backend() != backend:\n","        os.environ['KERAS_BACKEND'] = backend\n","        importlib.reload(K)\n","        assert K.backend() == backend\n","\n","set_keras_backend(\"tensorflow\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VKRx5PzuX-fm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":374},"outputId":"a83122f4-ca2e-4597-8dae-b4ec1da7837c","executionInfo":{"status":"ok","timestamp":1527854785017,"user_tz":-330,"elapsed":2490,"user":{"displayName":"Thiyagarajan Ramanathan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"111432628313695784552"}}},"cell_type":"code","source":["# define NMT model\n","from keras import backend as K\n","import keras.initializers as initializers\n","import keras.regularizers as regularizers\n","import keras.constraints as constraints\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation\n","import keras.layers as kl\n","def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n","    model = Sequential()\n","    model.add(kl.Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n","    model.add(kl.LSTM(n_units))\n","    model.add(kl.RepeatVector(tar_timesteps))\n","    model.add(kl.LSTM(n_units, return_sequences=True))\n","    model.add(AttentionDecoder(256,src_timesteps))\n","    model.add(kl.TimeDistributed(Dense(tar_vocab, activation='softmax')))\n","    return model\n","\n","# define model\n","model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","# summarize defined model\n","print(model.summary())"],"execution_count":40,"outputs":[{"output_type":"stream","text":["inputs shape: (?, ?, 256)\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 18, 256)           1083648   \n","_________________________________________________________________\n","lstm_5 (LSTM)                (None, 256)               525312    \n","_________________________________________________________________\n","repeat_vector_3 (RepeatVecto (None, 9, 256)            0         \n","_________________________________________________________________\n","lstm_6 (LSTM)                (None, 9, 256)            525312    \n","_________________________________________________________________\n","AttentionDecoder (AttentionD (None, 9, 18)             614486    \n","_________________________________________________________________\n","time_distributed_3 (TimeDist (None, 9, 7497)           142443    \n","=================================================================\n","Total params: 2,891,201\n","Trainable params: 2,891,201\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"metadata":{"id":"EpN7aukBYCXE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":3451},"outputId":"0bd23963-654f-4562-a102-de050a2d325e","executionInfo":{"status":"ok","timestamp":1527863105864,"user_tz":-330,"elapsed":1414876,"user":{"displayName":"Thiyagarajan Ramanathan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"111432628313695784552"}}},"cell_type":"code","source":["from keras.callbacks import ModelCheckpoint\n","# fit model\n","filename = 'drive/Colab Notebooks/model-viet-attention.h5'\n","checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","model.fit(trainX, trainY, epochs=50, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"],"execution_count":47,"outputs":[{"output_type":"stream","text":["Train on 13000 samples, validate on 2000 samples\n","Epoch 1/50\n"," - 28s - loss: 2.2940 - val_loss: 4.2910\n","\n","Epoch 00001: val_loss improved from inf to 4.29098, saving model to drive/Colab Notebooks/model-viet-attention.h5\n","Epoch 2/50\n"," - 29s - loss: 2.2971 - val_loss: 4.3211\n","\n","Epoch 00002: val_loss did not improve from 4.29098\n","Epoch 3/50\n"," - 28s - loss: 2.2992 - val_loss: 4.3225\n","\n","Epoch 00003: val_loss did not improve from 4.29098\n","Epoch 4/50\n"," - 28s - loss: 2.2963 - val_loss: 4.3331\n","\n","Epoch 00004: val_loss did not improve from 4.29098\n","Epoch 5/50\n"," - 28s - loss: 2.2849 - val_loss: 4.3306\n","\n","Epoch 00005: val_loss did not improve from 4.29098\n","Epoch 6/50\n"," - 28s - loss: 2.2715 - val_loss: 4.3522\n","\n","Epoch 00006: val_loss did not improve from 4.29098\n","Epoch 7/50\n"," - 28s - loss: 2.2566 - val_loss: 4.3600\n","\n","Epoch 00007: val_loss did not improve from 4.29098\n","Epoch 8/50\n"," - 28s - loss: 2.2447 - val_loss: 4.3577\n","\n","Epoch 00008: val_loss did not improve from 4.29098\n","Epoch 9/50\n"," - 28s - loss: 2.2466 - val_loss: 4.3789\n","\n","Epoch 00009: val_loss did not improve from 4.29098\n","Epoch 10/50\n"," - 28s - loss: 2.2466 - val_loss: 4.3794\n","\n","Epoch 00010: val_loss did not improve from 4.29098\n","Epoch 11/50\n"," - 28s - loss: 2.2480 - val_loss: 4.4449\n","\n","Epoch 00011: val_loss did not improve from 4.29098\n","Epoch 12/50\n"," - 28s - loss: 2.2639 - val_loss: 4.4001\n","\n","Epoch 00012: val_loss did not improve from 4.29098\n","Epoch 13/50\n"," - 28s - loss: 2.2436 - val_loss: 4.3936\n","\n","Epoch 00013: val_loss did not improve from 4.29098\n","Epoch 14/50\n"," - 28s - loss: 2.2238 - val_loss: 4.3988\n","\n","Epoch 00014: val_loss did not improve from 4.29098\n","Epoch 15/50\n"," - 28s - loss: 2.2063 - val_loss: 4.4196\n","\n","Epoch 00015: val_loss did not improve from 4.29098\n","Epoch 16/50\n"," - 28s - loss: 2.1931 - val_loss: 4.4246\n","\n","Epoch 00016: val_loss did not improve from 4.29098\n","Epoch 17/50\n"," - 28s - loss: 2.1851 - val_loss: 4.4362\n","\n","Epoch 00017: val_loss did not improve from 4.29098\n","Epoch 18/50\n"," - 28s - loss: 2.1807 - val_loss: 4.4567\n","\n","Epoch 00018: val_loss did not improve from 4.29098\n","Epoch 19/50\n"," - 28s - loss: 2.1806 - val_loss: 4.4416\n","\n","Epoch 00019: val_loss did not improve from 4.29098\n","Epoch 20/50\n"," - 28s - loss: 2.1777 - val_loss: 4.4695\n","\n","Epoch 00020: val_loss did not improve from 4.29098\n","Epoch 21/50\n"," - 28s - loss: 2.1764 - val_loss: 4.4944\n","\n","Epoch 00021: val_loss did not improve from 4.29098\n","Epoch 22/50\n"," - 28s - loss: 2.1747 - val_loss: 4.4864\n","\n","Epoch 00022: val_loss did not improve from 4.29098\n","Epoch 23/50\n"," - 28s - loss: 2.1788 - val_loss: 4.5140\n","\n","Epoch 00023: val_loss did not improve from 4.29098\n","Epoch 24/50\n"," - 28s - loss: 2.1816 - val_loss: 4.4980\n","\n","Epoch 00024: val_loss did not improve from 4.29098\n","Epoch 25/50\n"," - 28s - loss: 2.1736 - val_loss: 4.4819\n","\n","Epoch 00025: val_loss did not improve from 4.29098\n","Epoch 26/50\n"," - 28s - loss: 2.1471 - val_loss: 4.4943\n","\n","Epoch 00026: val_loss did not improve from 4.29098\n","Epoch 27/50\n"," - 28s - loss: 2.1290 - val_loss: 4.5190\n","\n","Epoch 00027: val_loss did not improve from 4.29098\n","Epoch 28/50\n"," - 28s - loss: 2.1230 - val_loss: 4.5528\n","\n","Epoch 00028: val_loss did not improve from 4.29098\n","Epoch 29/50\n"," - 28s - loss: 2.1364 - val_loss: 4.5560\n","\n","Epoch 00029: val_loss did not improve from 4.29098\n","Epoch 30/50\n"," - 28s - loss: 2.1293 - val_loss: 4.5407\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Epoch 00030: val_loss did not improve from 4.29098\n","Epoch 31/50\n"," - 28s - loss: 2.1301 - val_loss: 4.5405\n","\n","Epoch 00031: val_loss did not improve from 4.29098\n","Epoch 32/50\n"," - 28s - loss: 2.1178 - val_loss: 4.5707\n","\n","Epoch 00032: val_loss did not improve from 4.29098\n","Epoch 33/50\n"," - 28s - loss: 2.1014 - val_loss: 4.5945\n","\n","Epoch 00033: val_loss did not improve from 4.29098\n","Epoch 34/50\n"," - 28s - loss: 2.0888 - val_loss: 4.5838\n","\n","Epoch 00034: val_loss did not improve from 4.29098\n","Epoch 35/50\n"," - 28s - loss: 2.0800 - val_loss: 4.6068\n","\n","Epoch 00035: val_loss did not improve from 4.29098\n","Epoch 36/50\n"," - 28s - loss: 2.0785 - val_loss: 4.6000\n","\n","Epoch 00036: val_loss did not improve from 4.29098\n","Epoch 37/50\n"," - 28s - loss: 2.0743 - val_loss: 4.6204\n","\n","Epoch 00037: val_loss did not improve from 4.29098\n","Epoch 38/50\n"," - 28s - loss: 2.0738 - val_loss: 4.6320\n","\n","Epoch 00038: val_loss did not improve from 4.29098\n","Epoch 39/50\n"," - 28s - loss: 2.0764 - val_loss: 4.6213\n","\n","Epoch 00039: val_loss did not improve from 4.29098\n","Epoch 40/50\n"," - 28s - loss: 2.0782 - val_loss: 4.6363\n","\n","Epoch 00040: val_loss did not improve from 4.29098\n","Epoch 41/50\n"," - 28s - loss: 2.0824 - val_loss: 4.6117\n","\n","Epoch 00041: val_loss did not improve from 4.29098\n","Epoch 42/50\n"," - 28s - loss: 2.0860 - val_loss: 4.6486\n","\n","Epoch 00042: val_loss did not improve from 4.29098\n","Epoch 43/50\n"," - 28s - loss: 2.0764 - val_loss: 4.6788\n","\n","Epoch 00043: val_loss did not improve from 4.29098\n","Epoch 44/50\n"," - 28s - loss: 2.0589 - val_loss: 4.6739\n","\n","Epoch 00044: val_loss did not improve from 4.29098\n","Epoch 45/50\n"," - 28s - loss: 2.0410 - val_loss: 4.6684\n","\n","Epoch 00045: val_loss did not improve from 4.29098\n","Epoch 46/50\n"," - 28s - loss: 2.0250 - val_loss: 4.6826\n","\n","Epoch 00046: val_loss did not improve from 4.29098\n","Epoch 47/50\n"," - 28s - loss: 2.0192 - val_loss: 4.6952\n","\n","Epoch 00047: val_loss did not improve from 4.29098\n","Epoch 48/50\n"," - 28s - loss: 2.0136 - val_loss: 4.6846\n","\n","Epoch 00048: val_loss did not improve from 4.29098\n","Epoch 49/50\n"," - 28s - loss: 2.0075 - val_loss: 4.7087\n","\n","Epoch 00049: val_loss did not improve from 4.29098\n","Epoch 50/50\n"," - 28s - loss: 2.0107 - val_loss: 4.7019\n","\n","Epoch 00050: val_loss did not improve from 4.29098\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f37d28c3128>"]},"metadata":{"tags":[]},"execution_count":47}]},{"metadata":{"id":"ijcKgWtreyR5","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","# load datasets\n","dataset = load_clean_sentences('drive/Colab Notebooks/english-german-both.pkl')\n","train = load_clean_sentences('drive/Colab Notebooks/english-german-train.pkl')\n","test = load_clean_sentences('drive/Colab Notebooks/english-german-test.pkl')\n","# prepare english tokenizer\n","eng_tokenizer = create_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:, 0])\n","# prepare german tokenizer\n","ger_tokenizer = create_tokenizer(dataset[:, 1])\n","ger_vocab_size = len(ger_tokenizer.word_index) + 1\n","ger_length = max_length(dataset[:, 1])\n","# prepare data\n","trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n","testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8B1rJKYYn9kx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# map an integer to a word\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aVQwHnv5oI6I","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# generate target given source sequence\n","import numpy as np\n","def predict_sequence(model, tokenizer, source):\n","\tprediction = model.predict(source, verbose=0)[0]\n","\tintegers = [np.argmax(vector) for vector in prediction]\n","\ttarget = list()\n","\tfor i in integers:\n","\t\tword = word_for_id(i, tokenizer)\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\ttarget.append(word)\n","\treturn ' '.join(target)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F4uox-Y8rA77","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":51},"outputId":"c179e46c-ccd8-456a-c398-e150551ae553","executionInfo":{"status":"ok","timestamp":1527409029625,"user_tz":-330,"elapsed":3781,"user":{"displayName":"Thiyagarajan Ramanathan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"111432628313695784552"}}},"cell_type":"code","source":["!pip3 install nltk"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n"],"name":"stdout"}]},{"metadata":{"id":"5QersrKbo4u2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from nltk.translate.bleu_score import corpus_bleu as bleu_score\n","# evaluate the skill of the model\n","def evaluate_model(model, tokenizer, sources, raw_dataset):\n","\tactual, predicted = list(), list()\n","\tfor i, source in enumerate(sources):\n","\t\t# translate encoded source text\n","\t\tsource = source.reshape((1, source.shape[0]))\n","\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n","\t\traw_target, raw_src = raw_dataset[i]\n","\t\tif i < 10:\n","\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n","\t\tactual.append(raw_target.split())\n","\t\tpredicted.append(translation.split())\n","\t# calculate BLEU score\n","\tprint('BLEU-1: %f' % bleu_score(actual, predicted, weights=(1.0, 0, 0, 0)))\n","\tprint('BLEU-2: %f' % bleu_score(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","\tprint('BLEU-3: %f' % bleu_score(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n","\tprint('BLEU-4: %f' % bleu_score(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e6MOLYADo7fb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# load datasets\n","dataset = load_clean_sentences('drive/Colab Notebooks/english-german-both.pkl')\n","train = load_clean_sentences('drive/Colab Notebooks/english-german-train.pkl')\n","test = load_clean_sentences('drive/Colab Notebooks/english-german-test.pkl')\n","# prepare english tokenizer\n","eng_tokenizer = create_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:, 0])\n","# prepare german tokenizer\n","ger_tokenizer = create_tokenizer(dataset[:, 1])\n","ger_vocab_size = len(ger_tokenizer.word_index) + 1\n","ger_length = max_length(dataset[:, 1])\n","# prepare data\n","trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n","testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n"," "],"execution_count":0,"outputs":[]},{"metadata":{"id":"Yz9bYIZ5BUM1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":272},"outputId":"ffacf0b0-deb8-4c41-e5b3-a57655b2dbb0"},"cell_type":"code","source":["print('train')\n","evaluate_model(model, eng_tokenizer, trainX[0:1000], train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["train\n","src=[họ cần biết đọc], target=[they do need to be able to read], predicted=[we do apost to to to to to]\n","src=[và nó là một thứ rất mạng lưới], target=[and this is a very meshy thing], predicted=[and it]\n","src=[toàn bộ những điều này chúng rất có hiệu lực], target=[so all this is very powerful], predicted=[and this we is very much]\n","src=[nó ngừng di chuyển], target=[it stopped moving], predicted=[it very different]\n","src=[họ giết cậu ấy], target=[they killed him], predicted=[they here to]\n","src=[anh ấy có bốt điện thoại], target=[he has a telephone booth], predicted=[they is a different thank]\n","src=[vì sao tôi nên biết điều này], target=[why should i know this], predicted=[what do i apost it]\n","src=[có những mô hình khác nhau], target=[there are different paradigms], predicted=[this is new thank]\n","src=[và đây là năng suất], target=[and here aposs productivity], predicted=[and this aposs different]\n","src=[nó đặc biệt như thế đó], target=[it is that specific], predicted=[it is it very]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"}]},{"metadata":{"id":"NWuzUntRCrU7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":340},"outputId":"5a113ad1-50d2-4ffa-cdad-201fd625b919","executionInfo":{"status":"ok","timestamp":1527852186793,"user_tz":-330,"elapsed":15910,"user":{"displayName":"Thiyagarajan Ramanathan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"111432628313695784552"}}},"cell_type":"code","source":["print('test')\n","evaluate_model(model, eng_tokenizer, testX[0:500], test)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["test\n","src=[miwa matreyek], target=[miwa matreyek], predicted=[billion]\n","src=[may mắn chăng hay đó là cơ hội], target=[is it luck is it chance], predicted=[it it just a very smells]\n","src=[và giờ là tiếng ghita rock], target=[and now a rockabilly guitar], predicted=[and it aposs the]\n","src=[đó là một thành công vang dội], target=[it was a huge success], predicted=[it aposs a thing]\n","src=[và đây apos bệnh nhân apos trong suốt kiểu mẫu của chúng ta], target=[our standard seethrough patient], predicted=[and this are to of of us]\n","src=[sự tiến hoá xảy ra theo cách đó], target=[evolution works that way], predicted=[all people happened happened]\n","src=[thiên nhiên thì lại rất khác], target=[nature works very differently], predicted=[there there are]\n","src=[cô ấy không phải là cô ấy mà là cây mao lương hoa vàng], target=[she is not she but a buttercup], predicted=[she was no is no no]\n","src=[được rồi], target=[all right], predicted=[all right]\n","src=[và bạn hiểu tại sao bạn hiểu tại sao], target=[and you see why you see why], predicted=[and you you know know know]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.056250\n","BLEU-2: 0.237171\n","BLEU-3: 0.421732\n","BLEU-4: 0.487002\n"],"name":"stdout"}]},{"metadata":{"id":"0pJYB_nZFMVM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}